import os
from extractor import extract_resume_text
from ocr_engine import extract_text_from_image
from brain import ResumeBrain

def main():
    # 1. Path to your local resume
    resume_path = "data/my_resume.pdf"
    job_desc_path = "data/job_screenshot.png"

    print(f"--- Step 1: Extracting text from {resume_path} and {job_desc_path} ---")
    raw_resume = extract_resume_text(resume_path)
    raw_job_text = extract_text_from_image(job_desc_path)

    # Provider selection: use OpenAI if key is set, otherwise Ollama
    api_key = os.environ.get("OPENAI_API_KEY")
    if api_key:
        print("--- Step 2: Initializing the AI Brain (OpenAI) ---")
        brain = ResumeBrain(provider="openai", api_key=api_key)
    else:
        print("--- Step 2: Initializing the AI Brain (Ollama) ---")
        brain = ResumeBrain(provider="ollama")
    
    print("--- Step 3: Structuring the Resume ---")
    # This calls your brain.py logic to talk to Ollama
    structured_resume = brain.structure_resume(raw_resume.raw_text)
    
    print("--- Step 4: Structuring the Job Description ---")
    structured_job_desc = brain.structure_job_description(raw_job_text)

    print("\n--- Success! Structured Resume Data: ---")
    print(f"Name: {structured_resume.name}")
    print(f"Skills Identified: {', '.join(structured_resume.skills)}")

    print("\n--- Success! Structured Job Description Data: ---")
    print(f"Job Title: {structured_job_desc.title}")
    print(f"Required Skills: {', '.join(structured_job_desc.required_skills)}")

    # Phase 2: Semantic Analysis
    print("\n--- Step 5: Running Semantic Analysis ---")
    analysis = brain.analyze_resume(structured_resume, structured_job_desc)

    print("\n" + "="*60)
    print("ANALYSIS RESULTS")
    print("="*60)

    print(f"\nüìä Overall Alignment Score: {analysis.overall_alignment_score:.1%}")

    print(f"\n‚úÖ Top Strengths ({len(analysis.strengths)} found):")
    for i, strength in enumerate(analysis.strengths[:5], 1):
        print(f"  {i}. {strength}")

    print(f"\nüéØ Semantic Matches ({len(analysis.matches)} found):")
    for i, match in enumerate(sorted(analysis.matches, key=lambda x: x.match_score, reverse=True)[:5], 1):
        print(f"  {i}. [{match.match_type.upper()}] '{match.resume_item}' ‚Üí '{match.job_requirement}'")
        print(f"     Score: {match.match_score:.2f} | {match.reasoning}")

    print(f"\n‚ö†Ô∏è  Keyword Gaps ({len(analysis.gaps)} found):")
    for i, gap in enumerate(sorted(analysis.gaps, key=lambda g: {"high": 3, "medium": 2, "low": 1}[g.importance], reverse=True)[:5], 1):
        print(f"  {i}. [{gap.importance.upper()}] {gap.missing_keyword}")
        print(f"     ‚Üí {gap.integration_suggestion}")

    print(f"\nüí° Recommendations:")
    for i, rec in enumerate(analysis.recommendations, 1):
        print(f"  {i}. {rec}")

    print("\n" + "="*60)

    # Save analysis results to file
    output_path = "cache/latest_analysis.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(analysis.model_dump_json(indent=2))
    print(f"\nüíæ Full analysis saved to: {output_path}")

    # Phase 3: Resume Tailoring
    print("\n--- Step 6: Tailoring Resume ---")
    tailored_resume = brain.tailor_resume(structured_resume, analysis)

    print("\n" + "="*60)
    print("TAILORED RESUME")
    print("="*60)

    print("\n‚ú® Updated Skills:")
    print(f"  {', '.join(tailored_resume.updated_skills)}")

    print("\nüìù Tailored Work History:")
    for exp in tailored_resume.tailored_work_history:
        print(f"\n  üè¢ {exp.company} - {exp.role} ({exp.duration})")
        for bullet in exp.tailored_bullet_points:
            print(f"    ‚Ä¢ {bullet}")
    
    print("\n" + "="*60)

    # Save tailored resume to file
    output_path = "cache/tailored_resume.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(tailored_resume.model_dump_json(indent=2))
    print(f"\nüíæ Full tailored resume saved to: {output_path}")

if __name__ == "__main__":
    main()